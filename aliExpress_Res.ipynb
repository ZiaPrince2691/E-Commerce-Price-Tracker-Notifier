{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64e3b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f8282f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    proxy_list = []\n",
    "\n",
    "    table_rows = soup.find('table' , class_='table table-striped table-bordered').find('tbody').find_all('tr')\n",
    "    \n",
    "    for row in table_rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        ip = cols[0].text\n",
    "        port = cols[1].text\n",
    "        https = cols[6].text\n",
    "        if https == \"yes\":\n",
    "            proxy_list.append(f\"{ip}:{port}\")\n",
    "\n",
    "    return proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5db11caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_proxies(proxies, test_url=\"https://httpbin.org/ip\", timeout=5):\n",
    "    working = []\n",
    "    for proxy in proxies:\n",
    "        try:\n",
    "            res = requests.get(test_url, proxies={\n",
    "                \"http\": f\"http://{proxy}\",\n",
    "                \"https\": f\"http://{proxy}\"\n",
    "            }, timeout=timeout)\n",
    "            if res.status_code == 200:\n",
    "                print(f\"[✅] Working: {proxy}\")\n",
    "                working.append(proxy)\n",
    "        except:\n",
    "            print(f\"[❌] Failed: {proxy}\")\n",
    "        time.sleep(1)\n",
    "    return working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e27710bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_rotation(url, proxy_list):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(10):\n",
    "        proxy = random.choice(proxy_list)\n",
    "        print(f\"Trying with proxy: {proxy}\")\n",
    "        try:\n",
    "            res = requests.get(url, proxies={\n",
    "                \"http\": f\"http://{proxy}\",\n",
    "                \"https\": f\"http://{proxy}\"\n",
    "            }, headers=headers, timeout=10)\n",
    "            if res.status_code == 200:\n",
    "                print(f\"[✅] Success with {proxy}\")\n",
    "                return res.text\n",
    "        except Exception as e:\n",
    "            print(f\"[⚠️] Failed with {proxy}: {e}\")\n",
    "    print(\"❌ All proxies failed.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dadf37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "prices = []\n",
    "links  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb000c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aliexpress_page_scraper(html):\n",
    "\n",
    "    soup = BeautifulSoup(html , 'html.parser')\n",
    "    products = soup.find_all('div' , class_ = 'kr_k3')\n",
    "\n",
    "    for product in products:\n",
    "\n",
    "        product_title = product.find('div', class_ = 'kr_av').get('title')\n",
    "        product_price = product.find('div', class_ = 'kr_kj').text\n",
    "        product_link  = product.find('a', class_ = 'kr_b in_is search-card-item').get('href')\n",
    "\n",
    "        titles.append(product_title)\n",
    "        prices.append(product_price)\n",
    "        links.append(product_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20bcb72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[❌] Failed: 51.81.245.3:17981\n",
      "[❌] Failed: 47.236.224.32:8080\n",
      "[✅] Working: 45.12.150.82:8080\n",
      "[❌] Failed: 139.59.34.209:8080\n",
      "[❌] Failed: 66.201.7.151:3128\n",
      "[❌] Failed: 37.27.63.151:29923\n",
      "[❌] Failed: 57.129.81.201:8080\n",
      "[❌] Failed: 72.10.160.174:3295\n",
      "[✅] Working: 77.90.28.184:1080\n",
      "[❌] Failed: 31.40.248.2:8080\n",
      "[❌] Failed: 103.3.58.162:3128\n",
      "[❌] Failed: 71.14.218.2:8080\n",
      "[✅] Working: 159.69.57.20:8880\n",
      "[❌] Failed: 8.219.97.248:80\n",
      "[❌] Failed: 200.174.198.86:8888\n",
      "[✅] Working: 185.234.65.66:1080\n",
      "[❌] Failed: 188.245.239.104:4001\n",
      "[❌] Failed: 68.183.63.141:8080\n",
      "[❌] Failed: 45.22.209.157:8888\n",
      "[❌] Failed: 197.44.247.35:3128\n"
     ]
    }
   ],
   "source": [
    "all_proxies = fetch_proxies()\n",
    "proxy_list = validate_proxies(all_proxies[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c331b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with proxy: 45.12.150.82:8080\n",
      "[✅] Success with 45.12.150.82:8080\n",
      "Trying with proxy: 185.234.65.66:1080\n",
      "[✅] Success with 185.234.65.66:1080\n",
      "Trying with proxy: 45.12.150.82:8080\n",
      "[✅] Success with 45.12.150.82:8080\n",
      "Trying with proxy: 45.12.150.82:8080\n",
      "[✅] Success with 45.12.150.82:8080\n",
      "Trying with proxy: 185.234.65.66:1080\n",
      "[✅] Success with 185.234.65.66:1080\n"
     ]
    }
   ],
   "source": [
    "total_pages = 5\n",
    "\n",
    "for page in range(1,total_pages+1):\n",
    "\n",
    "    url = f'https://www.aliexpress.com/w/wholesale-airpods.html?page={page}'\n",
    "    html = scrape_with_rotation(url , proxy_list)\n",
    "\n",
    "    aliexpress_page_scraper(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f98bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with proxy: 77.90.28.184:1080\n",
      "[✅] Success with 77.90.28.184:1080\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.aliexpress.com/w/wholesale-gaming-keyboards.html'\n",
    "html = scrape_with_rotation(url , proxy_list)\n",
    "with open('page_1.html' , 'w' , encoding='utf-8') as file:\n",
    "    file.write(html)\n",
    "    \n",
    "aliexpress_page_scraper(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29e28754",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html , 'html.parser')\n",
    "products = soup.find_all('div' , class_ = 'kr_k3')\n",
    "\n",
    "for product in products:\n",
    "\n",
    "    product_title = product.find('div', class_ = 'kr_av').get('title')\n",
    "    print(product_title)\n",
    "    product_price = product.find('div', class_ = 'kr_kj').text\n",
    "    product_link  = product.find('a', class_ = 'kr_b in_is search-card-item').get('href')\n",
    "\n",
    "    titles.append(product_title)\n",
    "    prices.append(product_price)\n",
    "    links.append(product_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "590c703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d563f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTitles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPrices\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLinks\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinks\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Unique\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Unique\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Unique\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Unique\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Titles' : titles,\n",
    "    'Prices' : prices,\n",
    "    'Links'  : links\n",
    "})\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Links'].str.contains('item')]\n",
    "df.reset_index( drop=True , inplace=True)\n",
    "df.index = range(1,len(df)+1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a529194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('aliexpress_products.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
